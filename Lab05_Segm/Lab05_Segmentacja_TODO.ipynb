{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 5: Segmentacja polipów — Architektura typu U-Net(Kvasir-SEG)\n",
    "\n",
    "W tym ćwiczeniu chcemy nauczyć się budowania i trenowania modeli **segmentacji semantycznej** z wykorzystaniem architektury **U-Net** oraz frameworka **PyTorch Lightning** na zbiorze danych medycznych **Kvasir-SEG** (segmentacja polipów jelitowych w obrazach endoskopowych).\n",
    "\n",
    "**Główne zagadnienia:**\n",
    "- Implementacja `LightningDataModule`: podział train/val/test, augmentacje spójne dla obrazu i maski.\n",
    "- Budowa architektury **U-Net 2D** z enkoderem i dekoderem oraz skip connections.\n",
    "- Funkcje straty dla segmentacji: **BCEWithLogitsLoss** i **Dice Loss**.\n",
    "- Metryki: **Dice Score** i **IoU** (Intersection over Union) z biblioteki torchmetrics.\n",
    "- Wykorzystanie **Transfer Learning** ResNet jako encoder w U-Net.\n",
    "\n",
    "**Dataset:** \n",
    "Użyjemy **Kvasir-SEG** – zbioru zawierającego obrazy endoskopowe jelita grubego wraz z maskami segmentacji polipów. Dataset ten został stworzony do oceny algorytmów segmentacji w endoskopii i zawiera około 1000 par obraz-maska. Dane zostaną **automatycznie pobrane** w notebooku. Skalujemy je również do rozdzielczości 224x224 aby ograniczyć czas potrzebny na uczenie sieci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Instalacja i importy\n",
    "\n",
    "Jeśli chcemy powtarzalnych wyników możemy ustawić stały SEED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b032856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu | Lightning: 2.5.5\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install pytorch-lightning torchmetrics scikit-image opencv-python pillow --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import os, zipfile, urllib.request, random, glob, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "\n",
    "# SEED = 42\n",
    "SEED = None\n",
    "def set_seed(s=SEED):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "if SEED is not None:\n",
    "    set_seed()\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device, '| Lightning:', pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead66112",
   "metadata": {},
   "source": [
    "## 1) Pobranie i przygotowanie Kvasir-SEG\n",
    "\n",
    "**Kvasir-SEG** to zbiór danych zawierający obrazy endoskopowe jelita grubego wraz z maskami segmentacji polipów. Poniższy fragment automatycznie pobiera zbiór i sprawdza liczbę pobranych plików. \n",
    "\n",
    "Należy zwrócić uwagę, że dla zadania segmentacji zarówno obrazy wejściowe jak i groundtruth są obrazami. W przypadku segmentacji binarnej (tło-obiekt) maska ma wartości binarne. Wymusza to delikatnie inne podejście do zarządzania i przetwarzania wstępnego takich danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158b881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pobieranie Kvasir-SEG (~150 MB)…\n",
      "Rozpakowywanie…\n",
      "Gotowe: data/data_kvasir/Kvasir-SEG\n",
      "Liczba obrazów/masek: 1000 1000\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path('./data/data_kvasir')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ZIP_URL = 'https://datasets.simula.no/downloads/kvasir-seg.zip'\n",
    "ZIP_PATH = DATA_ROOT / 'kvasir.zip'\n",
    "EXTRACT_DIR = DATA_ROOT / 'Kvasir-SEG'\n",
    "\n",
    "if not EXTRACT_DIR.exists():\n",
    "    print('Pobieranie Kvasir-SEG (~150 MB)…')\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    urllib.request.urlretrieve(ZIP_URL, ZIP_PATH)\n",
    "    print('Rozpakowywanie…')\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
    "        zf.extractall(DATA_ROOT)\n",
    "    print('Gotowe:', EXTRACT_DIR)\n",
    "else:\n",
    "    print('Dane już dostępne:', EXTRACT_DIR)\n",
    "\n",
    "IMG_DIR = EXTRACT_DIR / 'images'\n",
    "MSK_DIR = EXTRACT_DIR / 'masks'\n",
    "imgs = sorted(glob.glob(str(IMG_DIR / '*.jpg')))\n",
    "msks = sorted(glob.glob(str(MSK_DIR / '*.jpg')))\n",
    "print('Liczba obrazów/masek:', len(imgs), len(msks))\n",
    "assert len(imgs) == len(msks) and len(imgs) > 0, 'Brak danych lub brak par obraz–maska.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e0f9f",
   "metadata": {},
   "source": [
    "## Zadanie 1 – Dataset i DataModule dla segmentacji\n",
    "\n",
    "W tym zadaniu zaimplementujesz własny Dataset oraz DataModule dla danych segmentacyjnych. Kluczową różnicą w porównaniu do klasyfikacji jest to, że augmentacje zmieniające pozycję obiektów (wszystkie przekształcenia geometryczne) muszą być stosowane zarówno do obrazu jak i maski – np. jeśli obracamy obraz o 10 stopni, to maskę również musimy obrócić o dokładnie ten sam kąt.\n",
    "\n",
    "**Dataset dla segmentacji:**\n",
    "1. Zaimplementuj klasę `KvasirDataset`, która dziedziczy po `Dataset`.\n",
    "2. W metodzie `__init__` zapamiętaj ścieżki do obrazów (`img_paths`) i masek (`msk_paths`), oraz opcjonalny obiekt transform.\n",
    "3. W metodzie `__len__` zwróć liczbę obrazów.\n",
    "4. W metodzie `__getitem__`:\n",
    "   - Wybierz odpowiednie ścieżki z zapamiętanych list podczas inicjalizacji.\n",
    "   - Wczytaj obraz i maskę używając funkcji pomocniczej `load_pair`.\n",
    "   - Jeśli `self.transform` istnieje, zastosuj go do obu (obraz, maska).\n",
    "   - Zwróć parę (obraz, maska).\n",
    "\n",
    "**Transform dla segmentacji:**\n",
    "1. Zaimplementuj klasę `SegmentationTransform`, która będzie stosować te same transformacje do obrazu i maski. W przypadku segmentacji musimy sami ją zaimplementować, aby móć stosować spójną segmentację dla obu obrazów.\n",
    "2. W metodzie `__init__`:\n",
    "   - Zapamiętaj `size` (docelowy rozmiar obrazu).\n",
    "   - Zapamiętaj `augment` (flaga czy stosować augmentacje, domyślnie False).\n",
    "   - Stwórz obiekt do konwersji PIL do tensor: `transforms.ToTensor()`.\n",
    "   - Stwórz **dwa** obiekty do resize: `transforms.Resize(size, ...)`. Pierwszy z interpolacją BILINEAR dla obrazu, a drugi z interpolacją NEAREST dla maski. Jest to ważne, gdyż maska jest binarna i nie chcemy rozmywać jej krawędzi.\n",
    "   - Stwórz obiekt normalizacji: `transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])` – przesunie wartości z [0,1] do [-1,1].\n",
    "3. W metodzie `__call__(self, img, mask)`:\n",
    "   - Wykonaj resize dla obrazu i maski (na obrazach PIL).\n",
    "   - Skonwertuj oba do tensorów (wartości [0, 1]).\n",
    "   - Zbinaryzuj maskę: `mask = (mask > 0).float()` – chcemy, żeby maska zawiera tylko 0 i 1, ale miała typ zmiennoprzecinkowy.\n",
    "   - Jeśli `self.augment == True`:\n",
    "     - Z prawdopodobieństwem 50% wykonaj horizontal flip (dla obu).\n",
    "     - Z prawdopodobieństwem 50% wykonaj losową rotację o kąt z przedziału [-10, 10] stopni. Zrób to samo dla obu obrazów, ale z różną interpolacją. Wykorzystaj `transforms.functional.rotate` i podaj argument `interpolation`.\n",
    "   - Znormalizuj obraz (nie maskę) do [-1, 1].\n",
    "   - Zwróć `(img.float(), mask.float())`.\n",
    "\n",
    "**DataModule:**\n",
    "1. Zaimplementuj klasę `KvasirDataModule`, która dziedziczy po `LightningDataModule`.\n",
    "2. W metodzie `__init__`:\n",
    "   - Zapamiętaj listy obrazów i masek.\n",
    "   - Zapamiętaj `batch`, `nw` (num_workers), `train_split`, `val_split`, `img_size`.\n",
    "   - Stwórz dwa obiekty transformacji: `self.train_transform` (z augment=True) i `self.val_test_transform` (z augment=False) jako obiekty zaimplementowanej wcześniej klasy.\n",
    "3. W metodzie `setup(self, stage=None)`:\n",
    "   - Wylosuj indeksy i podziel dane na train/val/test zgodnie z `train_split` i `val_split`. W tym celu stwórz listę indeksów od 0 do liczby próbek, a następnie wymieszaj je za pomocą `np.random.shuffle`. Podziel wynikowy wektor na 3 częsci zgodnie za przekazanym podziałem na część treningową, walidacyjną i testową. Następnie podziel również odpowiednio ścieżki do plików zgodnie z przygotowanymi listami indeksów.\n",
    "   - Stwórz i zapamiętaj trzy datasety (treningowy, walidacyjny i testowy) z odpowiednimi transformacjami i zawierające próbki zgodnie w wcześniejszym losowaniem.\n",
    "   - Wyświetl rozmiary zbiorów.\n",
    "4. Zaimplementuj metody `train_dataloader()`, `val_dataloader()`, `test_dataloader()`:\n",
    "   - Zwróć DataLoader z odpowiednim datastem.\n",
    "   - Ustaw `shuffle=True` tylko dla train.\n",
    "   - Ustaw `pin_memory=True` jeśli CUDA dostępne.\n",
    "\n",
    "**Poza klasami:**\n",
    "1. Stwórz instancję `KvasirDataModule` z listami `imgs` i `msks` (były stworzone we wcześniejszej komórce), rozmiarem batcha i liczbą workerów. Wartości te trzeba dobrać na podstawie specyfikacji maszyny na której uruchamiany będzie trening.\n",
    "2. Wywołaj `dm.setup()`.\n",
    "3. Pobierz jeden batch z train loadera (`next(iter(dm.train_dataloader()))`) i wyświetl kształty. Powinny pojawić się kształty typu `(16, 3, 384, 384)` dla obrazów i `(16, 1, 384, 384)` dla masek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39e7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a354f6d9",
   "metadata": {},
   "source": [
    "## Zadanie 2 – Implementacja U-Net: architektura enkodera i dekodera\n",
    "\n",
    "W tym zadaniu zaimplementujesz architekturę **U-Net** – jedną z najpopularniejszych sieci do segmentacji medycznej. U-Net składa się z:\n",
    "- **Enkodera** (downsampling path): ekstraktuje cechy wysokiego poziomu, zmniejszając rozdzielczość.\n",
    "- **Dekodera** (upsampling path): odbudowuje rozdzielczość, tworząc maskę segmentacji.\n",
    "- **Skip connections**: połączenia z enkodera do dekodera na tym samym poziomie rozdzielczości – pozwalają zachować szczegóły przestrzenne.\n",
    "\n",
    "![U-Net architecture diagram](https://iq.opengenus.org/content/images/2021/12/1_ovEGmOI3bcCeauu8jEBzsg.png)\n",
    "\n",
    "**Blok DoubleConv:**\n",
    "1. Zaimplementuj klasę `DoubleConv`, która dziedziczy po `nn.Module`.\n",
    "2. Przyjmuje parametry: `in_ch` (kanały wejściowe), `out_ch` (kanały wyjściowe), `dropout` (prawdopodobieństwo dropout, domyślnie 0.0).\n",
    "Dropout to technika regularyzacji, która losowo \"wyłącza\" pewien procent neuronów podczas treningu. Zmusza to sieć do uczenia się bardziej uniwersalnych i odpornych reprezentacji, ponieważ nie może polegać na pojedynczych neuronach/kanałach. Będziemy wykorzystywać `Dropout2d`, który usuwa całe kanały w warstwach konwolucyjnych. Warstwy te pozwalają ograniczyć zjawisko przeuczenia (overfitting).\n",
    "3. Zbuduj sekwencję warstw:\n",
    "   - `Conv2d(in_ch, out_ch, kernel_size=3, padding=1)` – konwolucja bez zmiany rozmiaru.\n",
    "   - `BatchNorm2d(out_ch)` – normalizacja batch.\n",
    "   - `ReLU(inplace=True)` – aktywacja.\n",
    "   - Opcjonalnie: `Dropout2d(p=dropout)` jeśli dropout > 0.\n",
    "   - Powtórz: `Conv2d(out_ch, out_ch, 3, padding=1)`, `BatchNorm2d`, `ReLU`. Ewentualnie może tutaj również nastąpić zmiana liczby kanałów wyjściowych. Wtedy na wejściu podczas inicjalizacji musimy przekazać dodatkowy parametr.\n",
    "   - Opcjonalnie: ponownie `Dropout2d(p=dropout)` jeśli dropout > 0.\n",
    "4. W metodzie `forward(x)` przepuść dane przez zbudowaną sekwencję.\n",
    "\n",
    "**Model UNetSmall:**\n",
    "1. Zaimplementuj klasę `UNetSmall`, która dziedziczy po `nn.Module`.\n",
    "2. W `__init__` przyjmij: `in_ch1=3` (RGB), `out_ch=1` (maska binarna), `dropout=0.0`.\n",
    "3. **Encoder (ścieżka downsampling):**\n",
    "   - `DoubleConv(in_ch1, out_ch1, dropout)` – pierwszy blok (d1).\n",
    "   - `nn.MaxPool2d(2)` – pooling 2×2 (zmniejsza rozdzielczość o połowę) (p1).\n",
    "   - `DoubleConv(out_ch1, out_ch2, dropout)` – drugi blok (d2).\n",
    "   - `nn.MaxPool2d(2)` (p2).\n",
    "   - `DoubleConv(out_ch2, out_ch3, dropout)` – trzeci blok (d3).\n",
    "   - `nn.MaxPool2d(2)` (p3).\n",
    "4. **Bottleneck (najgłębsza część):**\n",
    "   - `DoubleConv(out_ch3, out_ch4, dropout)` – ewentualnie w tej warstwie można zwiększyć dropout względem innych warstw (b).\n",
    "5. **Decoder (ścieżka upsampling):**\n",
    "   - `nn.ConvTranspose2d(out_ch4, out_ch3, kernel_size=2, stride=2)` – upsampling (zwiększa rozdzielczość 2×) (u3).\n",
    "   - `DoubleConv(2*out_ch3, out_ch3, dropout)` – uwaga: 2*out_ch3 = out_ch3 (z poprzedniej warstwy) + out_ch3 (skip connection z enkodera) (c3).\n",
    "   - `nn.ConvTranspose2d(out_ch3, out_ch2, 2, 2)` (u2).\n",
    "   - `DoubleConv(2*out_ch2, out_ch2, dropout)` (c2).\n",
    "   - `nn.ConvTranspose2d(out_ch2, out_ch1, 2, 2)` (u1).\n",
    "   - `DoubleConv(2*out_ch1, out_ch1, dropout)` (c1).\n",
    "6. **Warstwa wyjściowa:**\n",
    "   - `nn.Conv2d(out_ch1, out_ch, kernel_size=1)` – konwolucja 1×1, zwraca logity (out).\n",
    "7. W metodzie `forward(x)`:\n",
    "   - **Encoder:** przepuść wejście przez `d1→p1→d2→p2→d3→p3→b`, zapamiętuj wyjścia z d1, d2 i d3, bo są potrzebne do skip connections.\n",
    "   - **Decoder:**\n",
    "     - Przepuść wyjście z bottleneck przez u3, potem skonkatenuj ze skip connection z d3 `torch.cat([u3, d3], dim=1)`, a następnie przepuść przez c3.\n",
    "     - Zrób do samo wykorzystując warstwy u2 i c2 oraz wyjście z d2.\n",
    "     - Zrób do samo wykorzystując warstwy u1 i c1 oraz wyjście z d1.\n",
    "   - Na koniec przepuść dane przez warstwę wyjściową out.\n",
    "\n",
    "**Dice Loss:**\n",
    "1. Zaimplementuj klasę `DiceLoss`, dziedziczącą po `nn.Module`.\n",
    "2. W `__init__(self, eps=1e-6)` zapamiętaj `eps=1e-6` (wartość dodawana dla stabilności numerycznej).\n",
    "3. DiceLoss pochodzi od współczynnika Dice (Dice coefficient, F1 dla segmentacji), który mierzy nakładanie się dwóch masek: `Dice = 2 * |P ∩ G| / (|P| + |G|)`, gdzie `P` to predykcja, `G` to maska, natomiast operator | | oznacza liczność zbioru. Zakres wartości to [0, 1], im większe — tym lepsze dopasowanie. `DiceLoss` wynosi `1 − Dice` (dla zadanie minimalizacji). Dodajemy małą wartość eps dla stabilności numerycznej do licznika i mianownika. Skupia się on na nakładaniu się obszarów, a nie na pojedynczych pikselach, co daje lepszą jakość predykcji maski.\n",
    "4. W `forward(self, logits, targets)`:\n",
    "   - Zastosuj sigmoid do logitów: `torch.sigmoid(logits)`.\n",
    "   - Upewnij się że targets są typu float.\n",
    "   - Oblicz licznik. W tym celu pomnóż wyniki sigmoidy z `targets`, a następnie zsumuj po wymiarach przestrzennych, pomnóż przez 2 i dodał `eps`.\n",
    "   - Oblicz mianownik symetrycznie do licznika, zgodnie z wzorem.\n",
    "   - Podziel otrzymane wartości, oblicz wartość średnią po wszystkich wymiarach (batch) i odejmij wynik od 1. Zwróć obliczoną wielkość.\n",
    "\n",
    "**Callbacks i Lightning Module:**\n",
    "1. Kod zawiera już gotową implementację `MetricsCallback` (do zbierania metryk).\n",
    "2. Konieczna jest jeszcze implementacja `LitUNet` (Lightning wrapper dla zaprejektowanej sieci z optymalizatorem, stratami i metrykami), która dziedziczy po `LightningModule`. Implementacja jest podobna do poprzedniego ćwiczenia.\n",
    "3. W metodzie `__init__(self, in_ch=3, lr=1e-3, dropout=0.0)`:\n",
    "   - Wywołaj `super().__init__()`.\n",
    "   - Zapisz hiperparametry: `self.save_hyperparameters()` – Lightning automatycznie zapisze je w checkpointach.\n",
    "   - Stwórz instancję stworzonej wcześniej sieci.\n",
    "   - Zdefiniuj i zapamiętaj funkcje straty:\n",
    "     - `nn.BCEWithLogitsLoss()` – Binary Cross-Entropy (działa na logitach, wewnętrznie aplikuje sigmoid).\n",
    "     - `DiceLoss()` – nasza implementacja Dice Loss.\n",
    "   - Zdefiniuj metryki (z torchmetrics):\n",
    "     - `BinaryJaccardIndex()` – IoU (Intersection over Union).\n",
    "     - `BinaryF1Score()` – F1/Dice Score. Odpowiednik zaimplementowanej fukncji straty, ale używa progowania prawdopodobieństwa, przez co nie można dla niej obliczyć gradientu.\n",
    "4. W metodzie `forward(self, x)`:\n",
    "   - Przepuść wejście przez sieć.\n",
    "5. W metodzie `configure_optimizers(self)`:\n",
    "   - Wykorzystaj optymalizator `Adam`.\n",
    "   - Wykorzystaj scheduler `CosineAnnealingLR`.\n",
    "   - Zwróć słownik zawierający optymalizator i scheduler.\n",
    "6. Zdefiniuj metody `training_step`, `validation_step` i `test_step`. Prawie wszystkie operacje są w nich takie sameg więc można je przenieść do funkcji pomocniczej.\n",
    "   - Rozpakuj batch (obrazy i maski).\n",
    "   - Oblicz wyjście sieci `self(x)`.\n",
    "   - Oblicz stratę jako sumę BCEWithLogitsLoss + DiceLoss\n",
    "   - Oblicz metryki do monitoringu w bloku `with torch.no_grad():`:\n",
    "     - Wykonaj progowanie predykcji sieci, aby wyznaczyć wynik segmentacji.\n",
    "     - Upewnij się że zarówno maska groundtruth i wynik predykcji są typu `int`.\n",
    "     - Oblicz IoU i F1(Dice) za pomocą stworzonych podczas inicjalizacji obiektów.\n",
    "   - Zaloguj metryki za pomocą funkcji `self.log()`:\n",
    "     - Pierwszym argumentem jest nazwa logowanej metryki.\n",
    "     - Drugim argumentem jest jej wartość.\n",
    "     - Flaga `on_epoch` sprawia, że metryka jest agregowana i logowana dla całej epoki. Ustaw ją jako `True`.\n",
    "     - Flaga `prog_bar` mówi czy wartość tej metryki ma być wyświetlona obok paska postępu.\n",
    "   - Zwróć `loss` dla `training_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb98b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(pl.Callback):\n",
    "    \"\"\"Callback to collect training metrics for visualization\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_dice': [],\n",
    "            'val_dice': [],\n",
    "            'train_iou': [],\n",
    "            'val_iou': [],\n",
    "            'epoch': []\n",
    "        }\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Collect training metrics\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch + 1\n",
    "        \n",
    "        self.metrics['epoch'].append(epoch)\n",
    "        self.metrics['train_loss'].append(metrics.get('train_loss_epoch', float('nan')).item())\n",
    "        self.metrics['train_dice'].append(metrics.get('train_dice_epoch', float('nan')).item())\n",
    "        self.metrics['train_iou'].append(metrics.get('train_iou_epoch', float('nan')).item())\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Collect validation metrics\n",
    "        metrics = trainer.callback_metrics\n",
    "        \n",
    "        # Only add validation metrics if we have them\n",
    "        if 'val_loss' in metrics:\n",
    "            self.metrics['val_loss'].append(metrics['val_loss'].item())\n",
    "            self.metrics['val_dice'].append(metrics['val_dice'].item())\n",
    "            self.metrics['val_iou'].append(metrics['val_iou'].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c5b3d",
   "metadata": {},
   "source": [
    "## Zadanie 3 – Trening U-Net: callbacks, wizualizacja metryk i ewaluacja\n",
    "\n",
    "Teraz chcemy wytrenować stworzoną architekturę sieci do segmentacji.\n",
    "\n",
    "1. Stworzenie modelu i callbacków:\n",
    "   - Utwórz instancję klasy sieci.\n",
    "   - Utwórz obiekty `MetricsCallback` – będzie zbierać metryki do wizualizacji.\n",
    "   - Utwórz `EarlyStopping` – ma zatrzymać trening jeśli metryka F1/Dice na zbiorze walidacyjnym nie rośnie przez określoną liczbę epok.\n",
    "   - Utwórz `ModelCheckpoint` – ma zapisywać najlepszy model pod względem metryki F1/Dice dla zbioru walidacyjnego.\n",
    "\n",
    "2. **Trening:**\n",
    "   - Stwórz `Trainer(max_epochs=50, accelerator='auto', devices=1, callbacks=[metrics_callback, early, ckpt], precision=16)`. Określ maksymalną liczbę epok, ustaw `accelerator` jako `'auto'`, przekaż callbacki w `callbacks`. Możesz również sprawdzić jak precyzja numeryczna wpływa na szybkość i wyniki uczenia. Jest ona przekazywana przez argument `precision`. Porównaj wartość 32 i 16 (stosowanie Automatic Mixed Precision).\n",
    "   - Wykonaj trening sieci `trainer.fit`.\n",
    "\n",
    "3. **Wizualizacja przebiegu treningu:**\n",
    "   - Wyciągnij zebrane metryki z `metrics_callback.metrics`. Jest to słownik zawierający nazwy pól jak w wywołaniach metod `self.log` sieci.\n",
    "   - Stwórz 3 wykresy:\n",
    "     -  Train Loss i Val Loss w funkcji epok.\n",
    "     -  Train Dice i Val F1/Dice w funkcji epok.\n",
    "     -  Train IoU i Val IoU w funkcji epok.\n",
    "\n",
    "4. **Test na zbiorze testowym:**\n",
    "   - Wczytaj najlepszy model.\n",
    "   - Sprawdź metryki na zbiorze testowym za pomocą metody `test`.\n",
    "\n",
    "5. **Wizualizacja predykcji:**\n",
    "   - Przełącz model w tryb ewaluacji.\n",
    "   - Pobierz jeden batch z test loadera: `xb, yb = next(iter(dm.test_dataloader()))`.\n",
    "   - Oblicz predykcje dla pobranego batcha wewnątrz bloku `torch.no_grad():`. Dodatkowo obliczone logity przepuść przez sigmoidę, aby otrzymać wartości prawdopodobieństwa.\n",
    "   - Zbinaryzuj predykcje.\n",
    "   - Dla kilku pierwszych przykładów wyświetl w rzędzie:\n",
    "     - Kolumna 1: Oryginalny obraz: `((xb[i].permute(1,2,0).numpy()*0.5)+0.5).clip(0,1)`.\n",
    "     - Kolumna 2: Maska GT: `yb[i,0]`.\n",
    "     - Kolumna 3: Predykcja: `preds[i,0]`.\n",
    "     - Kolumna 4: Obraz oryginalny z nałożoną predykcją (zmień kolor pikseli na czerwony):\n",
    "     `overlay = img.copy(); m = preds[i,0].numpy()>0.5; overlay[m] = [1.0, 0.0, 0.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c3ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93dd5cc1",
   "metadata": {},
   "source": [
    "## Zadanie 4 – Transfer Learning: U-Net z pre-trained ResNet Encoder\n",
    "\n",
    "Znowu wykorzystamy transfer learning do zbudowania sieci o większej skuteczności. Zamiast używać całej nauczonej wcześniej sieci wyciągniemy tylko kilka warstw i użyjemy ich w enkoderze modelu U-Net.\n",
    "\n",
    "1. **ResNetEncoder:**\n",
    "   - Zaimplementuj klasę `ResNetEncoder`, która dziedziczy po `nn.Module`.\n",
    "   - W `__init__` przyjmij: `pretrained=True` (czy ładować pretrenowane wagi), `freeze_encoder=False` (czy wczytane wagi mają być zamrożone).\n",
    "   - Wczytaj pretrained ResNet18: `models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)`.\n",
    "   - Wyciągnij warstwy enkodera:\n",
    "     - `resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool` – początkowe warstwy.\n",
    "     - `resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4` – bloki ResNet (64, 128, 256, 512 kanałów).\n",
    "   - Jeśli `freeze_encoder=True`, ustaw wszystkie parametry na `requires_grad=False` wewnątrz bloku `for param in self.parameters():`\n",
    "   - W metodzie `forward(x)` przepuść dane przez warstwy i zwróć 5 tensorów skip connections: `x1` (po conv1), `x2` (po layer1), `x3` (po layer2), `x4` (po layer3), `x5` (po layer4/bottleneck).\n",
    "\n",
    "2. **UNetResNet:**\n",
    "   - Zaimplementuj klasę `UNetResNet`, która dziedziczy po `nn.Module`.\n",
    "   - W `__init__` przyjmij: `pretrained=True`, `freeze_encoder=False`, `dropout`.\n",
    "   - Stwórz encoder: `self.encoder = ResNetEncoder(pretrained, freeze_encoder)`.\n",
    "   - Zbuduj decoder (podobnie jak w poprzednim zadaniu):\n",
    "     - `ConvTranspose2d` upsampling z bottleneck (u1).\n",
    "     - `DoubleConv` pamiętaj o konkatenacji wyjścia z u1 i skip connection z x4.\n",
    "     - `ConvTranspose2d` (u2).\n",
    "     - `DoubleConv` u2 + x3.\n",
    "     - `ConvTranspose2d` (u3).\n",
    "     - `DoubleConv` u3 + x2.\n",
    "     - `ConvTranspose2d` (u4).\n",
    "     - `DoubleConv` u4 + x1.\n",
    "     - `ConvTranspose2d` powrót do orygionalej rozdzielczości.\n",
    "     - `DoubleConv`.\n",
    "   - Warstwa wyjściowa `Conv2d` z jednym kanałem wyjściowym o rozmiarze 1x1.\n",
    "   - W metodzie `forward(x)`:\n",
    "     - Wywołaj encoder: `x1, x2, x3, x4, x5 = self.encoder(x)`.\n",
    "     - Przepuść przez decoder i skip connections.\n",
    "     - Zwróć logity.\n",
    "\n",
    "3. **LitUNetResNet:**\n",
    "   - Zaimplementuj Lightning wrapper analogicznie do `LitUNet`.\n",
    "   - Dodaj metody pomocnicze:\n",
    "     - `freeze_encoder()`: zamraża wagi enkodera (tylko decoder będzie trenowany).\n",
    "     - `unfreeze_encoder()`: odmraża encoder (cała sieć będzie trenowana).\n",
    "\n",
    "4. **Test architektury:**\n",
    "   - Stwórz instancję `UNetResNet`.\n",
    "   - Przepuść losowy tensor przez sieć i sprawdź kształt wyjścia.\n",
    "   - Wyświetl osobno liczbę wszystkich oraz liczbę uczonych parametrów `sum(p.numel() for p in model_tl.parameters() if p.requires_grad)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f928d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2d4899",
   "metadata": {},
   "source": [
    "## Zadanie 5 – Dwuetapowy Transfer Learning: Frozen Encoder\n",
    "\n",
    "Trening chcemy wykonać podobnie jak w poprzednim ćwiczeniu, czyli zaczynamy od zamrożonych wag z transfer learningu, a potem wykonujemy fine-tuning. Proces treningu jest taki sam jak dla naszej wcześniejszej sieci.\n",
    "\n",
    "**Trening z zamrożonym encoderem:**\n",
    "1. Stwórz obiekt klasy `LitUNetResNet` z pretrenowanymi i zamrożonymi wagami.\n",
    "2. Stwórz callbacki: `MetricsCallback`, `EarlyStopping`, `ModelCheckpoint`.\n",
    "3. Stwórz `Trainer`.\n",
    "4. Wywołaj trening sieci wywołując metodę `fit`\n",
    "5. Wyświetl najlepszy checkpoint i val_dice.\n",
    "6. Sprawdź oraz wyświetl metryki (3 wykresy: loss, dice, iou) podobnie jak we wcześniejszym treningu.\n",
    "7. Wczytaj najlepszy model sieci i wykonaj dla niego testy na zbiorze testowym.\n",
    "8. Wyświetl przykładowe predykcje (4 obrazy × 4 kolumny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f9fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3fb4997",
   "metadata": {},
   "source": [
    "## Zadanie 6 – Dwuetapowy Transfer Learning: Fine-tuning\n",
    "\n",
    "**Fine-tuning całej sieci:**\n",
    "1. Odmroź encoder dla najlepszego modelu z poporzedniej części.\n",
    "2. Ustaw learning rate: `best_stage1.hparams.lr = ` (podczas fine-tuningu zazwyczaj jest mniejszy).\n",
    "3. Zaktualizuj hparam: `best_stage1.hparams.freeze_encoder = False`.\n",
    "4. Stwórz nowe callbacki: `MetricsCallback`, `EarlyStopping`, `ModelCheckpoint`.\n",
    "5. Stwórz nowy `Trainer`.\n",
    "6. Wykonaj trening wywołując metodę `fit`.\n",
    "7. Sprawdź oraz wyświetl metryki (3 wykresy: loss, dice, iou) podobnie jak we wcześniejszym treningu.\n",
    "8. Wczytaj najlepszy model sieci i wykonaj dla niego testy na zbiorze testowym.\n",
    "9. Wyświetl przykładowe predykcje (4 obrazy × 4 kolumny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9189e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c28eac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie i wnioski\n",
    "\n",
    "Porównaj wyniki przeprowadzonych treningów sieci. Weź pod uwagę liczbę trenowanych parametrów.\n",
    "Odpowiedz również krótko na poniższe pytania. \n",
    "\n",
    "1. Czym segmentacja różni się od zadania klasyfikacji?\n",
    "\n",
    "2. Czym jest architektura U-Net? Czym ona się charakteryzuje?\n",
    "\n",
    "3. Czym jest funkcja straty DiceLoss?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PiAOM-2025W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
